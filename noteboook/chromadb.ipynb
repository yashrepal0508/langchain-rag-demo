{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07d66ca8",
   "metadata": {},
   "source": [
    "## RAG pipeline with chromadb and huggingface embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb840e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_core.documents import Document\n",
    "import numpy as np\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e1847b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 15 pages\n",
      "First page preview: Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "loader=PyMuPDFLoader(\"data/pdf/attention_is_all_you_need.pdf\")\n",
    "documents=loader.load()\n",
    "\n",
    "print(f\"Loaded {len(documents)} pages\")\n",
    "print(f\"First page preview: {documents[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12af27dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 162 chunks\n",
      "First chunk: Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "text_splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=30,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "splits=text_splitter.split_documents(documents)\n",
    "print(f\"Split into {len(splits)} chunks\")\n",
    "print(f\"First chunk: {splits[0].page_content[:200]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7aa857d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6473a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store=Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f55b78f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 5 documents\n",
      "\n",
      "--- Document 1 ---\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez∗†\n",
      "University of \n",
      "\n",
      "--- Document 2 ---\n",
      "[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\n",
      "summarization. arXiv preprint arXiv:1705.04304, 2017.\n",
      "[29] Slav Petrov, Leon Barrett, Romain Thibaux, and\n",
      "\n",
      "--- Document 3 ---\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "\n",
      "\n",
      "--- Document 4 ---\n",
      "heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\n",
      "and semantic structure of the sentences.\n",
      "5\n",
      "Training\n",
      "This section describes the training regime \n",
      "\n",
      "--- Document 5 ---\n",
      "Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,\n",
      "pages 152–159. ACL, June 2006.\n",
      "[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposa\n"
     ]
    }
   ],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",  # or \"mmr\" for Maximum Marginal Relevance\n",
    "    search_kwargs={\"k\": 5}\n",
    ")\n",
    "\n",
    "query = \"what is the main topic?\"\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "\n",
    "print(f\"Retrieved {len(retrieved_docs)} documents\")\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\n--- Document {i+1} ---\")\n",
    "    print(doc.page_content[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f98fac51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMR Results (more diverse):\n",
      "\n",
      "--- Result 1 ---\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez∗†\n",
      "University of \n",
      "\n",
      "--- Result 2 ---\n",
      "[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\n",
      "summarization. arXiv preprint arXiv:1705.04304, 2017.\n",
      "[29] Slav Petrov, Leon Barrett, Romain Thibaux, and\n",
      "\n",
      "--- Result 3 ---\n",
      "1: Long Papers), pages 434–443. ACL, August 2013.\n",
      "12\n",
      "\n",
      "--- Result 4 ---\n",
      "architectures [38, 24, 15].\n",
      "Recurrent models typically factor computation along the symbol positions of the input and output\n",
      "sequences. Aligning the positions to steps in computation time, they genera\n",
      "\n",
      "--- Result 5 ---\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\n",
      "Full attentions for head 5. Bottom: Isolated attentions from just th\n"
     ]
    }
   ],
   "source": [
    "# MMR balances relevance with diversity\n",
    "retriever_mmr = vector_store.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5}\n",
    ")\n",
    "\n",
    "results_mmr = retriever_mmr.invoke(query)\n",
    "\n",
    "print(\"MMR Results (more diverse):\")\n",
    "for i, doc in enumerate(results_mmr):\n",
    "    print(f\"\\n--- Result {i+1} ---\")\n",
    "    print(doc.page_content[:200])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802f9c75",
   "metadata": {},
   "source": [
    "### Modern RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ef94f683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0.7,\n",
    "    groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f1be4a2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x11d7199a0>, search_kwargs={'k': 5})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"k\": 5}\n",
    ")\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7f8b7e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt template created\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Create a prompt template for RAG\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Keep the answer concise.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Answer:\"\"\"\n",
    ")\n",
    "\n",
    "print(\"Prompt template created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1b89c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG chain created successfully\n"
     ]
    }
   ],
   "source": [
    "# RAG LCEL\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Create a function to format and print retrieved docs\n",
    "def format_docs(docs):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RETRIEVED DOCUMENTS:\")\n",
    "    print(\"=\"*80)\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        print(f\"\\n--- Document {i} ---\")\n",
    "        print(doc.page_content[:300] + \"...\")\n",
    "        print(f\"Metadata: {doc.metadata}\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Build the chain with debugging\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt_template\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"RAG chain created successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b620dd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 1: QUESTION\n",
      "================================================================================\n",
      "What is the transformer architecture?\n",
      "\n",
      "================================================================================\n",
      "STEP 2: RETRIEVING DOCUMENTS\n",
      "================================================================================\n",
      "Retrieved 5 documents\n",
      "\n",
      "--- Document 1 ---\n",
      "Figure 1: The Transformer - model architecture.\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
      "connected layers for both the encoder and decoder, ...\n",
      "\n",
      "--- Document 2 ---\n",
      "∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n",
      "the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Tra...\n",
      "\n",
      "--- Document 3 ---\n",
      "relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
      "The Transformer allows for significantly more parallelization and can reach a new state of the art in\n",
      "t...\n",
      "\n",
      "--- Document 4 ---\n",
      "in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\n",
      "it more difficult to learn dependencies between distant positions [12]. In the Transformer this is\n",
      "r...\n",
      "\n",
      "--- Document 5 ---\n",
      "model by multiplying the training time, the number of GPUs used, and an estimate of the sustained\n",
      "single-precision floating-point capacity of each GPU 5.\n",
      "6.2\n",
      "Model Variations\n",
      "To evaluate the importanc...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "STEP 3: FORMATTED PROMPT\n",
      "================================================================================\n",
      "messages=[HumanMessage(content=\"You are an assistant for question-answering tasks. \\nUse the following pieces of retrieved context to answer the question. \\nIf you don't know the answer, just say that you don't know. \\nKeep the answer concise.\\n\\nQuestion: What is the transformer architecture?\\n\\nContext: Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\n\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\n\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\n\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\n\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2\\nModel Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\n\\nAnswer:\", additional_kwargs={}, response_metadata={})]\n",
      "\n",
      "================================================================================\n",
      "STEP 4: FINAL ANSWER\n",
      "================================================================================\n",
      "The transformer architecture is a model that uses stacked self-attention and fully connected layers for both the encoder and decoder, relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Step-by-step with full visibility\n",
    "question = \"What is the transformer architecture?\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: QUESTION\")\n",
    "print(\"=\"*80)\n",
    "print(question)\n",
    "\n",
    "# Step 2: Retrieve documents\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: RETRIEVING DOCUMENTS\")\n",
    "print(\"=\"*80)\n",
    "retrieved_docs = retriever.invoke(question)\n",
    "print(f\"Retrieved {len(retrieved_docs)} documents\\n\")\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"--- Document {i} ---\")\n",
    "    print(doc.page_content[:200] + \"...\")\n",
    "    print()\n",
    "\n",
    "# Step 3: Format context\n",
    "context = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "# Step 4: Create prompt\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: FORMATTED PROMPT\")\n",
    "print(\"=\"*80)\n",
    "formatted_prompt = prompt_template.invoke({\"question\": question, \"context\": context})\n",
    "print(formatted_prompt)\n",
    "\n",
    "# Step 5: Get LLM response\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 4: FINAL ANSWER\")\n",
    "print(\"=\"*80)\n",
    "response = llm.invoke(formatted_prompt)\n",
    "print(response.content)\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b27e15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
